{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_idx_data_cv(revs, word_idx_map, cv, max_l, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data_0, train_labels_0 = [],[]\n",
    "    train_data_1, train_labels_1 = [],[]\n",
    "    test_data, test_labels = [], []\n",
    "    \n",
    "    for rev in revs:\n",
    "        # convert sentence to 1 * 64 (represent by number)\n",
    "        sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, filter_h)\n",
    "        label = [0] * 2\n",
    "        label[rev[\"y\"]] = 1\n",
    "        if rev[\"split\"] == cv:\n",
    "            test_data.append(sent)\n",
    "            test_labels.append(label)\n",
    "        else:\n",
    "            if rev[\"y\"] == 0:\n",
    "                train_data_0.append(sent)\n",
    "                train_labels_0.append(label)\n",
    "            elif rev[\"y\"] == 1:\n",
    "                train_data_1.append(sent)\n",
    "                train_labels_1.append(label)\n",
    "    return [train_data_0, train_labels_0, train_data_1, train_labels_1, test_data, test_labels]\n",
    "\n",
    "\n",
    "def get_idx_from_sent(sent, word_idx_map, max_l, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    pad = filter_h - 1\n",
    "    for i in xrange(pad):\n",
    "        x.append(0)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l + 2 * pad:\n",
    "        x.append(0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# maximuam of count is 8\n",
    "def create_data(train_data_0, train_labels_0, train_data_1, train_labels_1,count):\n",
    "    train_data, train_labels = [], []\n",
    "    length = len(train_labels_1)\n",
    "    for i in range(length):\n",
    "        train_data.append(train_data_1[i])\n",
    "        train_labels.append(train_labels_1[i])\n",
    "        \n",
    "    if (count + 1) * length < len(train_data_0) or (count + 1) * length == len(train_data_0):\n",
    "        for i in range(length):\n",
    "            train_data.append(train_data_0[i + count * length])\n",
    "            train_labels.append(train_labels_0[i + count * length])\n",
    "    elif count * length < len(train_data_0):\n",
    "        prev = len(train_data_0) - count * length\n",
    "        post = length - prev\n",
    "        for i in range(prev):\n",
    "            train_data.append(train_data_0[i + count * length])\n",
    "            train_labels.append(train_labels_0[i + count * length]) \n",
    "        for i in range(post):\n",
    "            train_data.append(train_data_0[i])\n",
    "            train_labels.append(train_labels_0[i]) \n",
    "    \n",
    "    train_data = np.array(train_data, dtype=\"int\")\n",
    "    train_labels = np.asarray(train_labels, dtype=np.float32)\n",
    "    \n",
    "    shuffle_indices = np.random.permutation(np.arange(len(train_data)))\n",
    "    train_data = train_data[shuffle_indices]\n",
    "    train_labels = train_labels[shuffle_indices]\n",
    "    return train_data, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "x = cPickle.load(open(\"data/processed/tweet.p\", \"rb\"))\n",
    "revs, embedding, W2, word_idx_map, vocab, max_l = x[0], x[1], x[2], x[3], x[4], x[5]\n",
    "# all list, Need to convert to numpy\n",
    "train_data_0, train_labels_0, train_data_1, train_labels_1, test_data, test_labels = make_idx_data_cv(revs, word_idx_map, 1, max_l, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5496"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, train_labels = create_data(train_data_0, train_labels_0, train_data_1, train_labels_1, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_data(self, revs, word_idx_map, i, max_l, num_classes=2):\n",
    "    train_data, train_labels, test_data, test_labels = self.make_idx_data_cv(revs, word_idx_map, i, max_l, 5)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(train_data)))\n",
    "    train_data = train_data[shuffle_indices]\n",
    "    train_labels = train_labels[shuffle_indices]\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "# maximuam of count is 8\n",
    "def create_data(train_data_0, train_labels_0, train_data_1, train_labels_1,count):\n",
    "    train_data, train_labels = [], []\n",
    "    length = len(train_labels_1)\n",
    "    for i in range(length):\n",
    "        train_data.append(train_data_1[i])\n",
    "        train_labels.append(train_labels_1[i])\n",
    "        \n",
    "    if (count + 1) * length < len(train_data_0) or (count + 1) * length == len(train_data_0):\n",
    "        for i in range(length):\n",
    "            train_data.append(train_data_0[i + count * length])\n",
    "            train_labels.append(train_labels_0[i + count * length])\n",
    "    elif count * length < len(train_data_0):\n",
    "        prev = len(train_data_0) - count * length\n",
    "        post = length - prev\n",
    "        for i in range(prev):\n",
    "            train_data.append(train_data_0[i + count * length])\n",
    "            train_labels.append(train_labels_0[i + count * length]) \n",
    "        for i in range(post):\n",
    "            train_data.append(train_data_0[i])\n",
    "            train_labels.append(train_labels_0[i]) \n",
    "    \n",
    "    train_data = np.array(train_data, dtype=\"int\")\n",
    "    train_labels = np.asarray(train_labels, dtype=np.float32)\n",
    "    \n",
    "    shuffle_indices = np.random.permutation(np.arange(len(train_data)))\n",
    "    train_data = train_data[shuffle_indices]\n",
    "    train_labels = train_labels[shuffle_indices]\n",
    "    return train_data, train_labels\n",
    "\n",
    "def make_idx_data_cv(self, revs, word_idx_map, cv, max_l, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train_data_0, train_labels_0 = [],[]\n",
    "    train_data_1, train_labels_1 = [],[]\n",
    "    test_data, test_labels = [], []\n",
    "    for rev in revs:\n",
    "        # convert sentence to 1 * 64 (represent by number)\n",
    "        sent = self.get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, filter_h)\n",
    "        label = [0] * self.num_classes\n",
    "        label[rev[\"y\"]] = 1\n",
    "        if rev[\"split\"] == cv:\n",
    "            test_data.append(sent)\n",
    "            test_labels.append(label)\n",
    "        else:\n",
    "            if rev[\"y\"] == 0:\n",
    "                train_data_0.append(sent)\n",
    "                train_labels_0.append(label)\n",
    "            elif rev[\"y\"] == 1:\n",
    "                train_data_1.append(sent)\n",
    "                train_labels_1.append(label)\n",
    "    return [train_data_0, train_labels_0, train_data_1, train_labels_1, test_data, test_labels]\n",
    "\n",
    "\n",
    "def get_idx_from_sent(self, sent, word_idx_map, max_l, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    pad = filter_h - 1\n",
    "    for i in xrange(pad):\n",
    "        x.append(0)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l + 2 * pad:\n",
    "        x.append(0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(self, revs, embedding, word_idx_map, max_l, k_fold=10):\n",
    "    with tf.Session(graph=self.graph) as sess: \n",
    "        # Initialization\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        sess.run(self.embedding_init, feed_dict={self.embedding_placeholder: embedding})\n",
    "        # Training steps\n",
    "        batch_size = 50\n",
    "        num_epochs = 15\n",
    "        # 10662 / 50 = 213.24\n",
    "        steps_per_epoche = int(len(train_labels) / batch_size)\n",
    "        # 1066\n",
    "        num_steps = steps_per_epoche * num_epochs\n",
    "        epoch_num = 0\n",
    "        learning_rate_decay = 1\n",
    "        # train_data is unbalance ratio of 0:1 is 10\n",
    "        # for i in range(10):\n",
    "            \n",
    "        \n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # Shuffle the data in each epoch\n",
    "            if (step % steps_per_epoche == 0):\n",
    "                shuffle_indices = np.random.permutation(np.arange(len(train_data)))\n",
    "                train_data = train_data[shuffle_indices]\n",
    "                train_labels = train_labels[shuffle_indices]\n",
    "                print(\"epoche number %d\" % epoch_num)\n",
    "                # Get test results on each epoch\n",
    "                feed_dict = {self.input_x: test_data, self.input_y: test_labels, self.dropout_keep_prob: 1.0}\n",
    "                accuracy_out = sess.run(self.accuracy, feed_dict=feed_dict)\n",
    "                print('Test accuracy: %.3f' % accuracy_out)\n",
    "                # On each 8 epochs we decay the learning rate\n",
    "                if (epoch_num == 8):\n",
    "                    learning_rate_decay = learning_rate_decay * 0.5\n",
    "                if (epoch_num == 16):\n",
    "                    learning_rate_decay = learning_rate_decay * 0.1\n",
    "                epoch_num += 1\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_data[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Setting all placeholders\n",
    "            feed_dict = {self.input_x: batch_data, self.input_y: batch_labels, self.dropout_keep_prob: 0.5,\n",
    "                         self.tf_learning_rate_decay: learning_rate_decay}\n",
    "            _, l, accuracy_out = sess.run(\n",
    "                [self.optimizer, self.loss, self.accuracy], feed_dict=feed_dict)\n",
    "            \n",
    "        # Testing\n",
    "        feed_dict = {self.input_x: test_data, self.input_y: test_labels, self.dropout_keep_prob: 1.0}\n",
    "        accuracy_out = sess.run(self.accuracy, feed_dict=feed_dict)\n",
    "        print('Test accuracy: %.3f' % accuracy_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "        # Export model\n",
    "            # WARNING(break-tutorial-inline-code): The following code snippet is\n",
    "            # in-lined in tutorials, please update tutorial documents accordingly\n",
    "            # whenever code changes.\n",
    "            export_path_base = sys.argv[-1]\n",
    "            export_path = os.path.join(\n",
    "                    tf.compat.as_bytes(export_path_base),\n",
    "                    tf.compat.as_bytes(str(FLAGS.model_version)))\n",
    "            # print 'Exporting trained model to', export_path\n",
    "            builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "\n",
    "            # Build the signature_def_map.\n",
    "            classification_inputs = tf.saved_model.utils.build_tensor_info(\n",
    "                    serialized_tf_example)\n",
    "            classification_outputs_classes = tf.saved_model.utils.build_tensor_info(\n",
    "                    prediction_classes)\n",
    "            classification_outputs_scores = tf.saved_model.utils.build_tensor_info(values)\n",
    "\n",
    "            classification_signature = (\n",
    "                    tf.saved_model.signature_def_utils.build_signature_def(\n",
    "                            inputs={\n",
    "                                    tf.saved_model.signature_constants.CLASSIFY_INPUTS:\n",
    "                                            classification_inputs\n",
    "                            },\n",
    "                            outputs={\n",
    "                                    tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES:\n",
    "                                            classification_outputs_classes,\n",
    "                                    tf.saved_model.signature_constants.CLASSIFY_OUTPUT_SCORES:\n",
    "                                            classification_outputs_scores\n",
    "                            },\n",
    "                            method_name=tf.saved_model.signature_constants.CLASSIFY_METHOD_NAME))\n",
    "\n",
    "            tensor_info_x = tf.saved_model.utils.build_tensor_info(x)\n",
    "            tensor_info_y = tf.saved_model.utils.build_tensor_info(y)\n",
    "\n",
    "            prediction_signature = (\n",
    "                    tf.saved_model.signature_def_utils.build_signature_def(\n",
    "                            inputs={'images': tensor_info_x},\n",
    "                            outputs={'scores': tensor_info_y},\n",
    "                            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n",
    "\n",
    "            legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n",
    "            builder.add_meta_graph_and_variables(\n",
    "                    sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                    signature_def_map={\n",
    "                            'predict_images':\n",
    "                                    prediction_signature,\n",
    "                            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
    "                                    classification_signature,\n",
    "                    },\n",
    "                    legacy_init_op=legacy_init_op)\n",
    "\n",
    "            builder.save()\n",
    "\n",
    "            # print 'Done exporting!'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
